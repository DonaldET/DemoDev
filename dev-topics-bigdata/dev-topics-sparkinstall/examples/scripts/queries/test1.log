
 Summit Spark Queries Test 1 script using driver TestPart1.py on D:\util\spark-3.4.1-bin-hadoop3-scala2.13 executing $0.cmd
   -- Base Path  : D:\GitHub\DemoDev\dev-topics-bigdata\dev-topics-sparkinstall\examples\scripts\queries\
   -- Driver Path: D:\GitHub\DemoDev\dev-topics-bigdata\dev-topics-sparkinstall\examples\queries\
   -- Spark Home : D:\util\spark-3.4.1-bin-hadoop3-scala2.13
java version "17.0.6" 2023-01-17 LTS
Java(TM) SE Runtime Environment (build 17.0.6+9-LTS-190)
Java HotSpot(TM) 64-Bit Server VM (build 17.0.6+9-LTS-190, mixed mode, sharing)
The system cannot find the file specified.
 D:\util\spark-3.4.1-bin-hadoop3-scala2.13\bin\spark-submit D:\GitHub\DemoDev\dev-topics-bigdata\dev-topics-sparkinstall\examples\queries\TestPart1.py --py-files part1.py --files ..\..\queries\data\customers.csv, ..\..\queries\data\orders.csv, ..\..\queries\data\purchases.json
23/08/15 21:50:49 INFO SparkContext: Running Spark version 3.4.1
23/08/15 21:50:49 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
23/08/15 21:50:49 INFO ResourceUtils: ==============================================================
23/08/15 21:50:49 INFO ResourceUtils: No custom resources configured for spark.driver.
23/08/15 21:50:49 INFO ResourceUtils: ==============================================================
23/08/15 21:50:49 INFO SparkContext: Submitted application: part1_programming
23/08/15 21:50:49 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/08/15 21:50:49 INFO ResourceProfile: Limiting resource is cpu
23/08/15 21:50:49 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/08/15 21:50:49 INFO SecurityManager: Changing view acls to: Don
23/08/15 21:50:49 INFO SecurityManager: Changing modify acls to: Don
23/08/15 21:50:49 INFO SecurityManager: Changing view acls groups to: 
23/08/15 21:50:49 INFO SecurityManager: Changing modify acls groups to: 
23/08/15 21:50:49 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Don; groups with view permissions: EMPTY; users with modify permissions: Don; groups with modify permissions: EMPTY
23/08/15 21:50:50 INFO Utils: Successfully started service 'sparkDriver' on port 51123.
23/08/15 21:50:50 INFO SparkEnv: Registering MapOutputTracker
23/08/15 21:50:50 INFO SparkEnv: Registering BlockManagerMaster
23/08/15 21:50:50 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/08/15 21:50:50 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/08/15 21:50:50 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/08/15 21:50:50 INFO DiskBlockManager: Created local directory at D:\Temp\blockmgr-e7d6ae6a-a99a-4452-9fc5-7260bb34a9bf
23/08/15 21:50:50 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
23/08/15 21:50:50 INFO SparkEnv: Registering OutputCommitCoordinator
23/08/15 21:50:50 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
23/08/15 21:50:50 INFO Utils: Successfully started service 'SparkUI' on port 4040.
23/08/15 21:50:51 INFO Executor: Starting executor ID driver on host THOR.mshome.net
23/08/15 21:50:51 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/08/15 21:50:51 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51124.
23/08/15 21:50:51 INFO NettyBlockTransferService: Server created on THOR.mshome.net:51124
23/08/15 21:50:51 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/08/15 21:50:51 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, THOR.mshome.net, 51124, None)
23/08/15 21:50:51 INFO BlockManagerMasterEndpoint: Registering block manager THOR.mshome.net:51124 with 434.4 MiB RAM, BlockManagerId(driver, THOR.mshome.net, 51124, None)
23/08/15 21:50:51 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, THOR.mshome.net, 51124, None)
23/08/15 21:50:51 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, THOR.mshome.net, 51124, None)
Testing Spark Queries Part 1
Spark object: <pyspark.sql.session.SparkSession object at 0x000001228979E050>

<><><><> Check StructType Creation
StructType([StructField('id', StringType(), True), StructField('InsuranceProvider', StringType(), True), StructField('Type', StructType([StructField('Client', StructType([StructField('PaidIn', StructType([StructField('Insuranceid', StringType(), True), StructField('Insurancedesc', StringType(), True), StructField('purchaseditems', StructType([StructField('InsuranceLabel', StringType(), True), StructField('InsuranceNumber', StringType(), True), StructField('Insuranceprice', DoubleType(), True), StructField('Insurancequantity', IntegerType(), True), StructField('childItems', StructType([StructField('InsuranceLabel', StringType(), True), StructField('InsuranceNumber', StringType(), True), StructField('Insuranceprice', DoubleType(), True), StructField('Insurancequantity', IntegerType(), True), StructField('discountsreceived', StructType([StructField('amount', IntegerType(), True), StructField('description', StringType(), True)]), True)]), True), StructField('discountsreceived', StructType([StructField('amount', IntegerType(), True), StructField('description', StringType(), True)]), True)]), True)]), True)]), True)]), True), StructField('eventTime', StringType(), True)])
------------------

<><><><> Read purchases JSON file
root
 |-- InsuranceProvider: string (nullable = true)
 |-- Type: struct (nullable = true)
 |    |-- Client: struct (nullable = true)
 |    |    |-- PaidIn: struct (nullable = true)
 |    |    |    |-- Insurancedesc: string (nullable = true)
 |    |    |    |-- Insuranceid: string (nullable = true)
 |    |    |    |-- purchaseditems: array (nullable = true)
 |    |    |    |    |-- element: struct (containsNull = true)
 |    |    |    |    |    |-- InsuranceLabel: string (nullable = true)
 |    |    |    |    |    |-- InsuranceNumber: string (nullable = true)
 |    |    |    |    |    |-- Insuranceprice: long (nullable = true)
 |    |    |    |    |    |-- Insurancequantity: long (nullable = true)
 |    |    |    |    |    |-- childItems: array (nullable = true)
 |    |    |    |    |    |    |-- element: struct (containsNull = true)
 |    |    |    |    |    |    |    |-- InsuranceLabel: string (nullable = true)
 |    |    |    |    |    |    |    |-- InsuranceNumber: string (nullable = true)
 |    |    |    |    |    |    |    |-- Insuranceprice: long (nullable = true)
 |    |    |    |    |    |    |    |-- Insurancequantity: long (nullable = true)
 |    |    |    |    |    |    |    |-- discountsreceived: array (nullable = true)
 |    |    |    |    |    |    |    |    |-- element: struct (containsNull = true)
 |    |    |    |    |    |    |    |    |    |-- amount: long (nullable = true)
 |    |    |    |    |    |    |    |    |    |-- description: string (nullable = true)
 |    |    |    |    |    |-- discountsreceived: array (nullable = true)
 |    |    |    |    |    |    |-- element: struct (containsNull = true)
 |    |    |    |    |    |    |    |-- amount: long (nullable = true)
 |    |    |    |    |    |    |    |-- description: string (nullable = true)
 |-- eventTime: string (nullable = true)
 |-- id: string (nullable = true)

+-----------------+--------------------+--------------------+--------------------+
|InsuranceProvider|                Type|           eventTime|                  id|
+-----------------+--------------------+--------------------+--------------------+
|         Embroker|{{{Magic happens ...|2020-05-19T01:59:...|16465147863122345...|
+-----------------+--------------------+--------------------+--------------------+

------------------

<><><><> Read rows from array (purchases JSON file)
_____ SKIPPING _____
------------------

<><><><> Write CSV file
root
 |-- firstname: string (nullable = true)
 |-- middlename: string (nullable = true)
 |-- lastname: string (nullable = true)
 |-- id: string (nullable = true)
 |-- gender: string (nullable = true)
 |-- salary: integer (nullable = true)

 -- Display Test CSV
+---------+----------+--------+-----+------+------+
|firstname|middlename|lastname|id   |gender|salary|
+---------+----------+--------+-----+------+------+
|James    |          |Smith   |36636|M     |3000  |
|Michael  |Rose      |        |40288|M     |4000  |
|Robert   |          |Williams|42114|M     |4000  |
|Maria    |Anne      |Jones   |39192|F     |4000  |
|Jen      |Mary      |Brown   |     |F     |-1    |
+---------+----------+--------+-----+------+------+


HDFS Target before write
 'Checking HDFS Local File System'
 '--------------------'


HDFS Target after write
 'Checking HDFS Local File System'
D:\GitHub\DemoDev\dev-topics-bigdata\dev-topics-sparkinstall\examples\scripts\queries\tmp\spark_output\flattened_purchases
D:\GitHub\DemoDev\dev-topics-bigdata\dev-topics-sparkinstall\examples\scripts\queries\tmp\spark_output\flattened_purchases\.part-00000-d89f6a07-1169-47a0-9304-dee6f29634ea-c000.csv.crc
D:\GitHub\DemoDev\dev-topics-bigdata\dev-topics-sparkinstall\examples\scripts\queries\tmp\spark_output\flattened_purchases\._SUCCESS.crc
D:\GitHub\DemoDev\dev-topics-bigdata\dev-topics-sparkinstall\examples\scripts\queries\tmp\spark_output\flattened_purchases\part-00000-d89f6a07-1169-47a0-9304-dee6f29634ea-c000.csv
D:\GitHub\DemoDev\dev-topics-bigdata\dev-topics-sparkinstall\examples\scripts\queries\tmp\spark_output\flattened_purchases\_SUCCESS
 '--------------------'
------------------

<><><><> Check Table Creation
-- Create Table
23/08/15 21:51:17 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
23/08/15 21:51:17 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
23/08/15 21:51:22 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0
23/08/15 21:51:22 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore UNKNOWN@172.21.160.1
Table created
------------------

 Queries Tests 1 Script Done


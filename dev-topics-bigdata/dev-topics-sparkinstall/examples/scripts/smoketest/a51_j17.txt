
 Summit Spark Smoke Test $x_driverSPARK_HOMEx_script.cmd
   -- Base Path  : D:\GitHub\DemoDev\dev-topics-bigdata\dev-topics-sparkinstall\examples\scripts\smoketest\
   -- Driver Path: D:\GitHub\DemoDev\dev-topics-bigdata\dev-topics-sparkinstall\examples\smoketest\
   -- Spark Home : D:\util\spark-3.4.1-bin-hadoop3-scala2.13
 D:\util\spark-3.4.1-bin-hadoop3-scala2.13\bin\spark-submit --deploy-mode client D:\GitHub\DemoDev\dev-topics-bigdata\dev-topics-sparkinstall\examples\smoketest\smoketest.py
23/08/09 18:57:05 INFO SparkContext: Running Spark version 3.4.1
23/08/09 18:57:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/08/09 18:57:06 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
23/08/09 18:57:06 INFO ResourceUtils: ==============================================================
23/08/09 18:57:06 INFO ResourceUtils: No custom resources configured for spark.driver.
23/08/09 18:57:06 INFO ResourceUtils: ==============================================================
23/08/09 18:57:06 INFO SparkContext: Submitted application: CollectedSmokeTests
23/08/09 18:57:06 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/08/09 18:57:06 INFO ResourceProfile: Limiting resource is cpu
23/08/09 18:57:06 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/08/09 18:57:06 INFO SecurityManager: Changing view acls to: Don
23/08/09 18:57:06 INFO SecurityManager: Changing modify acls to: Don
23/08/09 18:57:06 INFO SecurityManager: Changing view acls groups to: 
23/08/09 18:57:06 INFO SecurityManager: Changing modify acls groups to: 
23/08/09 18:57:06 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Don; groups with view permissions: EMPTY; users with modify permissions: Don; groups with modify permissions: EMPTY
23/08/09 18:57:06 INFO Utils: Successfully started service 'sparkDriver' on port 50444.
23/08/09 18:57:06 INFO SparkEnv: Registering MapOutputTracker
23/08/09 18:57:06 INFO SparkEnv: Registering BlockManagerMaster
23/08/09 18:57:06 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/08/09 18:57:06 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/08/09 18:57:06 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/08/09 18:57:06 INFO DiskBlockManager: Created local directory at D:\Temp\blockmgr-c2db37e7-9697-443f-9466-3bf4cc151877
23/08/09 18:57:06 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
23/08/09 18:57:06 INFO SparkEnv: Registering OutputCommitCoordinator
23/08/09 18:57:06 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
23/08/09 18:57:06 INFO Utils: Successfully started service 'SparkUI' on port 4040.
23/08/09 18:57:06 INFO Executor: Starting executor ID driver on host 10.0.0.130
23/08/09 18:57:06 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/08/09 18:57:06 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50445.
23/08/09 18:57:06 INFO NettyBlockTransferService: Server created on 10.0.0.130:50445
23/08/09 18:57:06 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/08/09 18:57:06 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.0.130, 50445, None)
23/08/09 18:57:06 INFO BlockManagerMasterEndpoint: Registering block manager 10.0.0.130:50445 with 434.4 MiB RAM, BlockManagerId(driver, 10.0.0.130, 50445, None)
23/08/09 18:57:06 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.0.130, 50445, None)
23/08/09 18:57:06 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.0.130, 50445, None)

***************************
*** PySpark Smoke Tests ***
***************************
Smoke Test 2: Create a DataFrame
23/08/09 18:57:07 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
23/08/09 18:57:07 INFO SharedState: Warehouse path is 'file:/D:/GitHub/DemoDev/dev-topics-bigdata/dev-topics-sparkinstall/examples/scripts/smoketest/spark-warehouse'.
23/08/09 18:57:09 INFO CodeGenerator: Code generated in 131.0353 ms
23/08/09 18:57:09 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
23/08/09 18:57:09 INFO DAGScheduler: Got job 0 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
23/08/09 18:57:09 INFO DAGScheduler: Final stage: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0)
23/08/09 18:57:09 INFO DAGScheduler: Parents of final stage: List()
23/08/09 18:57:09 INFO DAGScheduler: Missing parents: List()
23/08/09 18:57:09 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
23/08/09 18:57:09 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 13.0 KiB, free 434.4 MiB)
23/08/09 18:57:09 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.7 KiB, free 434.4 MiB)
23/08/09 18:57:09 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.0.130:50445 (size: 6.7 KiB, free: 434.4 MiB)
23/08/09 18:57:09 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1535
23/08/09 18:57:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
23/08/09 18:57:09 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
23/08/09 18:57:09 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.0.0.130, executor driver, partition 0, PROCESS_LOCAL, 7481 bytes) 
23/08/09 18:57:09 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
23/08/09 18:57:09 INFO PythonRunner: Times: total = 550, boot = 519, init = 31, finish = 0
23/08/09 18:57:09 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1975 bytes result sent to driver
23/08/09 18:57:09 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 665 ms on 10.0.0.130 (executor driver) (1/1)
23/08/09 18:57:09 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
23/08/09 18:57:10 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 50446
23/08/09 18:57:10 INFO DAGScheduler: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0) finished in 0.808 s
23/08/09 18:57:10 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
23/08/09 18:57:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
23/08/09 18:57:10 INFO DAGScheduler: Job 0 finished: showString at NativeMethodAccessorImpl.java:0, took 0.835546 s
23/08/09 18:57:10 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
23/08/09 18:57:10 INFO DAGScheduler: Got job 1 (showString at NativeMethodAccessorImpl.java:0) with 4 output partitions
23/08/09 18:57:10 INFO DAGScheduler: Final stage: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0)
23/08/09 18:57:10 INFO DAGScheduler: Parents of final stage: List()
23/08/09 18:57:10 INFO DAGScheduler: Missing parents: List()
23/08/09 18:57:10 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
23/08/09 18:57:10 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 13.0 KiB, free 434.4 MiB)
23/08/09 18:57:10 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.7 KiB, free 434.4 MiB)
23/08/09 18:57:10 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.0.130:50445 (size: 6.7 KiB, free: 434.4 MiB)
23/08/09 18:57:10 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535
23/08/09 18:57:10 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(1, 2, 3, 4))
23/08/09 18:57:10 INFO TaskSchedulerImpl: Adding task set 1.0 with 4 tasks resource profile 0
23/08/09 18:57:10 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (10.0.0.130, executor driver, partition 1, PROCESS_LOCAL, 7481 bytes) 
23/08/09 18:57:10 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (10.0.0.130, executor driver, partition 2, PROCESS_LOCAL, 7481 bytes) 
23/08/09 18:57:10 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3) (10.0.0.130, executor driver, partition 3, PROCESS_LOCAL, 7536 bytes) 
23/08/09 18:57:10 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4) (10.0.0.130, executor driver, partition 4, PROCESS_LOCAL, 7481 bytes) 
23/08/09 18:57:10 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
23/08/09 18:57:10 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)
23/08/09 18:57:10 INFO Executor: Running task 2.0 in stage 1.0 (TID 3)
23/08/09 18:57:10 INFO Executor: Running task 3.0 in stage 1.0 (TID 4)
23/08/09 18:57:10 INFO PythonRunner: Times: total = 535, boot = 507, init = 28, finish = 0
23/08/09 18:57:11 INFO PythonRunner: Times: total = 1055, boot = 1023, init = 32, finish = 0
23/08/09 18:57:11 INFO PythonRunner: Times: total = 1576, boot = 1541, init = 35, finish = 0
23/08/09 18:57:12 INFO Executor: Finished task 2.0 in stage 1.0 (TID 3). 2043 bytes result sent to driver
23/08/09 18:57:12 INFO Executor: Finished task 3.0 in stage 1.0 (TID 4). 1975 bytes result sent to driver
23/08/09 18:57:12 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1975 bytes result sent to driver
23/08/09 18:57:12 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 2077 ms on 10.0.0.130 (executor driver) (1/4)
23/08/09 18:57:12 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 4) in 2077 ms on 10.0.0.130 (executor driver) (2/4)
23/08/09 18:57:12 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2079 ms on 10.0.0.130 (executor driver) (3/4)
23/08/09 18:57:12 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.0.0.130:50445 in memory (size: 6.7 KiB, free: 434.4 MiB)
23/08/09 18:57:12 INFO PythonRunner: Times: total = 2093, boot = 2062, init = 31, finish = 0
23/08/09 18:57:12 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 1975 bytes result sent to driver
23/08/09 18:57:12 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 2109 ms on 10.0.0.130 (executor driver) (4/4)
23/08/09 18:57:12 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
23/08/09 18:57:12 INFO DAGScheduler: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 2.118 s
23/08/09 18:57:12 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
23/08/09 18:57:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
23/08/09 18:57:12 INFO DAGScheduler: Job 1 finished: showString at NativeMethodAccessorImpl.java:0, took 2.120371 s
23/08/09 18:57:12 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
23/08/09 18:57:12 INFO DAGScheduler: Got job 2 (showString at NativeMethodAccessorImpl.java:0) with 11 output partitions
23/08/09 18:57:12 INFO DAGScheduler: Final stage: ResultStage 2 (showString at NativeMethodAccessorImpl.java:0)
23/08/09 18:57:12 INFO DAGScheduler: Parents of final stage: List()
23/08/09 18:57:12 INFO DAGScheduler: Missing parents: List()
23/08/09 18:57:12 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
23/08/09 18:57:12 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 13.0 KiB, free 434.4 MiB)
23/08/09 18:57:12 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 6.7 KiB, free 434.4 MiB)
23/08/09 18:57:12 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.0.130:50445 (size: 6.7 KiB, free: 434.4 MiB)
23/08/09 18:57:12 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1535
23/08/09 18:57:12 INFO DAGScheduler: Submitting 11 missing tasks from ResultStage 2 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15))
23/08/09 18:57:12 INFO TaskSchedulerImpl: Adding task set 2.0 with 11 tasks resource profile 0
23/08/09 18:57:12 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 5) (10.0.0.130, executor driver, partition 5, PROCESS_LOCAL, 7481 bytes) 
23/08/09 18:57:12 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 6) (10.0.0.130, executor driver, partition 6, PROCESS_LOCAL, 7481 bytes) 
23/08/09 18:57:12 INFO TaskSetManager: Starting task 2.0 in stage 2.0 (TID 7) (10.0.0.130, executor driver, partition 7, PROCESS_LOCAL, 7537 bytes) 
23/08/09 18:57:12 INFO TaskSetManager: Starting task 3.0 in stage 2.0 (TID 8) (10.0.0.130, executor driver, partition 8, PROCESS_LOCAL, 7481 bytes) 
23/08/09 18:57:12 INFO TaskSetManager: Starting task 4.0 in stage 2.0 (TID 9) (10.0.0.130, executor driver, partition 9, PROCESS_LOCAL, 7481 bytes) 
23/08/09 18:57:12 INFO TaskSetManager: Starting task 5.0 in stage 2.0 (TID 10) (10.0.0.130, executor driver, partition 10, PROCESS_LOCAL, 7481 bytes) 
23/08/09 18:57:12 INFO TaskSetManager: Starting task 6.0 in stage 2.0 (TID 11) (10.0.0.130, executor driver, partition 11, PROCESS_LOCAL, 7540 bytes) 
23/08/09 18:57:12 INFO TaskSetManager: Starting task 7.0 in stage 2.0 (TID 12) (10.0.0.130, executor driver, partition 12, PROCESS_LOCAL, 7481 bytes) 
23/08/09 18:57:12 INFO TaskSetManager: Starting task 8.0 in stage 2.0 (TID 13) (10.0.0.130, executor driver, partition 13, PROCESS_LOCAL, 7481 bytes) 
23/08/09 18:57:12 INFO TaskSetManager: Starting task 9.0 in stage 2.0 (TID 14) (10.0.0.130, executor driver, partition 14, PROCESS_LOCAL, 7481 bytes) 
23/08/09 18:57:12 INFO TaskSetManager: Starting task 10.0 in stage 2.0 (TID 15) (10.0.0.130, executor driver, partition 15, PROCESS_LOCAL, 7536 bytes) 
23/08/09 18:57:12 INFO Executor: Running task 3.0 in stage 2.0 (TID 8)
23/08/09 18:57:12 INFO Executor: Running task 1.0 in stage 2.0 (TID 6)
23/08/09 18:57:12 INFO Executor: Running task 2.0 in stage 2.0 (TID 7)
23/08/09 18:57:12 INFO Executor: Running task 0.0 in stage 2.0 (TID 5)
23/08/09 18:57:12 INFO Executor: Running task 4.0 in stage 2.0 (TID 9)
23/08/09 18:57:12 INFO Executor: Running task 5.0 in stage 2.0 (TID 10)
23/08/09 18:57:12 INFO Executor: Running task 6.0 in stage 2.0 (TID 11)
23/08/09 18:57:12 INFO Executor: Running task 7.0 in stage 2.0 (TID 12)
23/08/09 18:57:12 INFO Executor: Running task 10.0 in stage 2.0 (TID 15)
23/08/09 18:57:12 INFO Executor: Running task 9.0 in stage 2.0 (TID 14)
23/08/09 18:57:12 INFO Executor: Running task 8.0 in stage 2.0 (TID 13)
23/08/09 18:57:12 INFO PythonRunner: Times: total = 536, boot = 508, init = 28, finish = 0
23/08/09 18:57:13 INFO PythonRunner: Times: total = 1049, boot = 1020, init = 29, finish = 0
23/08/09 18:57:13 INFO PythonRunner: Times: total = 1571, boot = 1538, init = 33, finish = 0
23/08/09 18:57:14 INFO PythonRunner: Times: total = 2082, boot = 2054, init = 28, finish = 0
23/08/09 18:57:14 INFO PythonRunner: Times: total = 2597, boot = 2570, init = 27, finish = 0
23/08/09 18:57:15 INFO PythonRunner: Times: total = 3099, boot = 3069, init = 30, finish = 0
23/08/09 18:57:15 INFO PythonRunner: Times: total = 3591, boot = 3563, init = 28, finish = 0
23/08/09 18:57:16 INFO PythonRunner: Times: total = 4100, boot = 4073, init = 27, finish = 0
23/08/09 18:57:16 INFO PythonRunner: Times: total = 4618, boot = 4590, init = 28, finish = 0
23/08/09 18:57:17 INFO PythonRunner: Times: total = 5139, boot = 5105, init = 34, finish = 0
23/08/09 18:57:17 INFO Executor: Finished task 5.0 in stage 2.0 (TID 10). 1975 bytes result sent to driver
23/08/09 18:57:17 INFO Executor: Finished task 1.0 in stage 2.0 (TID 6). 1975 bytes result sent to driver
23/08/09 18:57:17 INFO TaskSetManager: Finished task 5.0 in stage 2.0 (TID 10) in 5605 ms on 10.0.0.130 (executor driver) (1/11)
23/08/09 18:57:17 INFO Executor: Finished task 0.0 in stage 2.0 (TID 5). 1975 bytes result sent to driver
23/08/09 18:57:17 INFO Executor: Finished task 9.0 in stage 2.0 (TID 14). 1932 bytes result sent to driver
23/08/09 18:57:17 INFO Executor: Finished task 3.0 in stage 2.0 (TID 8). 1975 bytes result sent to driver
23/08/09 18:57:17 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 6) in 5606 ms on 10.0.0.130 (executor driver) (2/11)
23/08/09 18:57:17 INFO Executor: Finished task 10.0 in stage 2.0 (TID 15). 2000 bytes result sent to driver
23/08/09 18:57:17 INFO Executor: Finished task 6.0 in stage 2.0 (TID 11). 2047 bytes result sent to driver
23/08/09 18:57:17 INFO Executor: Finished task 7.0 in stage 2.0 (TID 12). 1932 bytes result sent to driver
23/08/09 18:57:17 INFO Executor: Finished task 2.0 in stage 2.0 (TID 7). 2047 bytes result sent to driver
23/08/09 18:57:17 INFO TaskSetManager: Finished task 3.0 in stage 2.0 (TID 8) in 5606 ms on 10.0.0.130 (executor driver) (3/11)
23/08/09 18:57:17 INFO Executor: Finished task 8.0 in stage 2.0 (TID 13). 1975 bytes result sent to driver
23/08/09 18:57:17 INFO TaskSetManager: Finished task 9.0 in stage 2.0 (TID 14) in 5605 ms on 10.0.0.130 (executor driver) (4/11)
23/08/09 18:57:17 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 5) in 5607 ms on 10.0.0.130 (executor driver) (5/11)
23/08/09 18:57:17 INFO TaskSetManager: Finished task 10.0 in stage 2.0 (TID 15) in 5606 ms on 10.0.0.130 (executor driver) (6/11)
23/08/09 18:57:17 INFO TaskSetManager: Finished task 6.0 in stage 2.0 (TID 11) in 5607 ms on 10.0.0.130 (executor driver) (7/11)
23/08/09 18:57:17 INFO TaskSetManager: Finished task 7.0 in stage 2.0 (TID 12) in 5607 ms on 10.0.0.130 (executor driver) (8/11)
23/08/09 18:57:17 INFO TaskSetManager: Finished task 2.0 in stage 2.0 (TID 7) in 5608 ms on 10.0.0.130 (executor driver) (9/11)
23/08/09 18:57:17 INFO TaskSetManager: Finished task 8.0 in stage 2.0 (TID 13) in 5606 ms on 10.0.0.130 (executor driver) (10/11)
23/08/09 18:57:17 INFO PythonRunner: Times: total = 5636, boot = 5595, init = 41, finish = 0
23/08/09 18:57:17 INFO Executor: Finished task 4.0 in stage 2.0 (TID 9). 1975 bytes result sent to driver
23/08/09 18:57:17 INFO TaskSetManager: Finished task 4.0 in stage 2.0 (TID 9) in 5654 ms on 10.0.0.130 (executor driver) (11/11)
23/08/09 18:57:17 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
23/08/09 18:57:17 INFO DAGScheduler: ResultStage 2 (showString at NativeMethodAccessorImpl.java:0) finished in 5.661 s
23/08/09 18:57:17 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
23/08/09 18:57:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
23/08/09 18:57:17 INFO DAGScheduler: Job 2 finished: showString at NativeMethodAccessorImpl.java:0, took 5.663737 s
23/08/09 18:57:17 INFO CodeGenerator: Code generated in 21.5267 ms
+---------+--------+-------+-----+
|firstname|lastname|country|state|
+---------+--------+-------+-----+
|    James|   Smith|    USA|   CA|
|  Michael|    Rose|    USA|   NY|
|   Robert|Williams|    USA|   CA|
|    Maria|   Jones|    USA|   FL|
+---------+--------+-------+-----+


root
 |-- firstname: string (nullable = true)
 |-- lastname: string (nullable = true)
 |-- country: string (nullable = true)
 |-- state: string (nullable = true)


23/08/09 18:57:17 INFO SparkContext: Starting job: collect at D:\GitHub\DemoDev\dev-topics-bigdata\dev-topics-sparkinstall\examples\smoketest\smoketest.py:21
23/08/09 18:57:17 INFO DAGScheduler: Got job 3 (collect at D:\GitHub\DemoDev\dev-topics-bigdata\dev-topics-sparkinstall\examples\smoketest\smoketest.py:21) with 16 output partitions
23/08/09 18:57:17 INFO DAGScheduler: Final stage: ResultStage 3 (collect at D:\GitHub\DemoDev\dev-topics-bigdata\dev-topics-sparkinstall\examples\smoketest\smoketest.py:21)
23/08/09 18:57:17 INFO DAGScheduler: Parents of final stage: List()
23/08/09 18:57:17 INFO DAGScheduler: Missing parents: List()
23/08/09 18:57:17 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[8] at collect at D:\GitHub\DemoDev\dev-topics-bigdata\dev-topics-sparkinstall\examples\smoketest\smoketest.py:21), which has no missing parents
23/08/09 18:57:17 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 13.0 KiB, free 434.3 MiB)
23/08/09 18:57:17 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.7 KiB, free 434.3 MiB)
23/08/09 18:57:17 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.0.130:50445 (size: 6.7 KiB, free: 434.4 MiB)
23/08/09 18:57:17 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1535
23/08/09 18:57:17 INFO DAGScheduler: Submitting 16 missing tasks from ResultStage 3 (MapPartitionsRDD[8] at collect at D:\GitHub\DemoDev\dev-topics-bigdata\dev-topics-sparkinstall\examples\smoketest\smoketest.py:21) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
23/08/09 18:57:17 INFO TaskSchedulerImpl: Adding task set 3.0 with 16 tasks resource profile 0
23/08/09 18:57:17 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 16) (10.0.0.130, executor driver, partition 0, PROCESS_LOCAL, 7481 bytes) 
23/08/09 18:57:17 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 17) (10.0.0.130, executor driver, partition 1, PROCESS_LOCAL, 7481 bytes) 
23/08/09 18:57:17 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 18) (10.0.0.130, executor driver, partition 2, PROCESS_LOCAL, 7481 bytes) 
23/08/09 18:57:17 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 19) (10.0.0.130, executor driver, partition 3, PROCESS_LOCAL, 7536 bytes) 
23/08/09 18:57:17 INFO TaskSetManager: Starting task 4.0 in stage 3.0 (TID 20) (10.0.0.130, executor driver, partition 4, PROCESS_LOCAL, 7481 bytes) 
23/08/09 18:57:17 INFO TaskSetManager: Starting task 5.0 in stage 3.0 (TID 21) (10.0.0.130, executor driver, partition 5, PROCESS_LOCAL, 7481 bytes) 
23/08/09 18:57:17 INFO TaskSetManager: Starting task 6.0 in stage 3.0 (TID 22) (10.0.0.130, executor driver, partition 6, PROCESS_LOCAL, 7481 bytes) 
23/08/09 18:57:17 INFO TaskSetManager: Starting task 7.0 in stage 3.0 (TID 23) (10.0.0.130, executor driver, partition 7, PROCESS_LOCAL, 7537 bytes) 
23/08/09 18:57:17 INFO TaskSetManager: Starting task 8.0 in stage 3.0 (TID 24) (10.0.0.130, executor driver, partition 8, PROCESS_LOCAL, 7481 bytes) 
23/08/09 18:57:17 INFO TaskSetManager: Starting task 9.0 in stage 3.0 (TID 25) (10.0.0.130, executor driver, partition 9, PROCESS_LOCAL, 7481 bytes) 
23/08/09 18:57:17 INFO TaskSetManager: Starting task 10.0 in stage 3.0 (TID 26) (10.0.0.130, executor driver, partition 10, PROCESS_LOCAL, 7481 bytes) 
23/08/09 18:57:17 INFO TaskSetManager: Starting task 11.0 in stage 3.0 (TID 27) (10.0.0.130, executor driver, partition 11, PROCESS_LOCAL, 7540 bytes) 
23/08/09 18:57:17 INFO TaskSetManager: Starting task 12.0 in stage 3.0 (TID 28) (10.0.0.130, executor driver, partition 12, PROCESS_LOCAL, 7481 bytes) 
23/08/09 18:57:17 INFO TaskSetManager: Starting task 13.0 in stage 3.0 (TID 29) (10.0.0.130, executor driver, partition 13, PROCESS_LOCAL, 7481 bytes) 
23/08/09 18:57:17 INFO TaskSetManager: Starting task 14.0 in stage 3.0 (TID 30) (10.0.0.130, executor driver, partition 14, PROCESS_LOCAL, 7481 bytes) 
23/08/09 18:57:17 INFO TaskSetManager: Starting task 15.0 in stage 3.0 (TID 31) (10.0.0.130, executor driver, partition 15, PROCESS_LOCAL, 7536 bytes) 
23/08/09 18:57:17 INFO Executor: Running task 1.0 in stage 3.0 (TID 17)
23/08/09 18:57:17 INFO Executor: Running task 2.0 in stage 3.0 (TID 18)
23/08/09 18:57:17 INFO Executor: Running task 0.0 in stage 3.0 (TID 16)
23/08/09 18:57:17 INFO Executor: Running task 3.0 in stage 3.0 (TID 19)
23/08/09 18:57:17 INFO Executor: Running task 4.0 in stage 3.0 (TID 20)
23/08/09 18:57:17 INFO Executor: Running task 5.0 in stage 3.0 (TID 21)
23/08/09 18:57:17 INFO Executor: Running task 6.0 in stage 3.0 (TID 22)
23/08/09 18:57:17 INFO Executor: Running task 8.0 in stage 3.0 (TID 24)
23/08/09 18:57:17 INFO Executor: Running task 7.0 in stage 3.0 (TID 23)
23/08/09 18:57:17 INFO Executor: Running task 9.0 in stage 3.0 (TID 25)
23/08/09 18:57:17 INFO Executor: Running task 10.0 in stage 3.0 (TID 26)
23/08/09 18:57:17 INFO Executor: Running task 11.0 in stage 3.0 (TID 27)
23/08/09 18:57:17 INFO Executor: Running task 12.0 in stage 3.0 (TID 28)
23/08/09 18:57:17 INFO Executor: Running task 14.0 in stage 3.0 (TID 30)
23/08/09 18:57:17 INFO Executor: Running task 13.0 in stage 3.0 (TID 29)
23/08/09 18:57:17 INFO Executor: Running task 15.0 in stage 3.0 (TID 31)
23/08/09 18:57:18 INFO PythonRunner: Times: total = 554, boot = 527, init = 27, finish = 0
23/08/09 18:57:18 INFO PythonRunner: Times: total = 1057, boot = 1028, init = 29, finish = 0
23/08/09 18:57:19 INFO PythonRunner: Times: total = 1577, boot = 1549, init = 28, finish = 0
23/08/09 18:57:19 INFO PythonRunner: Times: total = 2098, boot = 2070, init = 28, finish = 0
23/08/09 18:57:20 INFO PythonRunner: Times: total = 2601, boot = 2569, init = 32, finish = 0
23/08/09 18:57:20 INFO PythonRunner: Times: total = 3092, boot = 3064, init = 28, finish = 0
23/08/09 18:57:21 INFO PythonRunner: Times: total = 3618, boot = 3590, init = 28, finish = 0
23/08/09 18:57:22 INFO PythonRunner: Times: total = 4138, boot = 4106, init = 32, finish = 0
23/08/09 18:57:22 INFO PythonRunner: Times: total = 4652, boot = 4623, init = 29, finish = 0
23/08/09 18:57:23 INFO PythonRunner: Times: total = 5141, boot = 5112, init = 29, finish = 0
23/08/09 18:57:23 INFO PythonRunner: Times: total = 5645, boot = 5617, init = 28, finish = 0
23/08/09 18:57:24 INFO PythonRunner: Times: total = 6149, boot = 6120, init = 29, finish = 0
23/08/09 18:57:24 INFO PythonRunner: Times: total = 6668, boot = 6636, init = 32, finish = 0
23/08/09 18:57:25 INFO PythonRunner: Times: total = 7180, boot = 7153, init = 27, finish = 0
23/08/09 18:57:25 INFO PythonRunner: Times: total = 7685, boot = 7657, init = 28, finish = 0
23/08/09 18:57:26 INFO Executor: Finished task 3.0 in stage 3.0 (TID 19). 2036 bytes result sent to driver
23/08/09 18:57:26 INFO Executor: Finished task 12.0 in stage 3.0 (TID 28). 1968 bytes result sent to driver
23/08/09 18:57:26 INFO Executor: Finished task 2.0 in stage 3.0 (TID 18). 1925 bytes result sent to driver
23/08/09 18:57:26 INFO Executor: Finished task 14.0 in stage 3.0 (TID 30). 2011 bytes result sent to driver
23/08/09 18:57:26 INFO Executor: Finished task 6.0 in stage 3.0 (TID 22). 2011 bytes result sent to driver
23/08/09 18:57:26 INFO Executor: Finished task 9.0 in stage 3.0 (TID 25). 1968 bytes result sent to driver
23/08/09 18:57:26 INFO Executor: Finished task 4.0 in stage 3.0 (TID 20). 1968 bytes result sent to driver
23/08/09 18:57:26 INFO Executor: Finished task 1.0 in stage 3.0 (TID 17). 2054 bytes result sent to driver
23/08/09 18:57:26 INFO Executor: Finished task 10.0 in stage 3.0 (TID 26). 2011 bytes result sent to driver
23/08/09 18:57:26 INFO Executor: Finished task 11.0 in stage 3.0 (TID 27). 2040 bytes result sent to driver
23/08/09 18:57:26 INFO Executor: Finished task 8.0 in stage 3.0 (TID 24). 2054 bytes result sent to driver
23/08/09 18:57:26 INFO Executor: Finished task 7.0 in stage 3.0 (TID 23). 2040 bytes result sent to driver
23/08/09 18:57:26 INFO Executor: Finished task 15.0 in stage 3.0 (TID 31). 2036 bytes result sent to driver
23/08/09 18:57:26 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 10.0.0.130:50445 in memory (size: 6.7 KiB, free: 434.4 MiB)
23/08/09 18:57:26 INFO TaskSetManager: Finished task 3.0 in stage 3.0 (TID 19) in 8182 ms on 10.0.0.130 (executor driver) (1/16)
23/08/09 18:57:26 INFO Executor: Finished task 5.0 in stage 3.0 (TID 21). 1968 bytes result sent to driver
23/08/09 18:57:26 INFO Executor: Finished task 13.0 in stage 3.0 (TID 29). 1968 bytes result sent to driver
23/08/09 18:57:26 INFO TaskSetManager: Finished task 12.0 in stage 3.0 (TID 28) in 8180 ms on 10.0.0.130 (executor driver) (2/16)
23/08/09 18:57:26 INFO TaskSetManager: Finished task 2.0 in stage 3.0 (TID 18) in 8183 ms on 10.0.0.130 (executor driver) (3/16)
23/08/09 18:57:26 INFO TaskSetManager: Finished task 6.0 in stage 3.0 (TID 22) in 8182 ms on 10.0.0.130 (executor driver) (4/16)
23/08/09 18:57:26 INFO TaskSetManager: Finished task 9.0 in stage 3.0 (TID 25) in 8182 ms on 10.0.0.130 (executor driver) (5/16)
23/08/09 18:57:26 INFO TaskSetManager: Finished task 4.0 in stage 3.0 (TID 20) in 8183 ms on 10.0.0.130 (executor driver) (6/16)
23/08/09 18:57:26 INFO TaskSetManager: Finished task 14.0 in stage 3.0 (TID 30) in 8181 ms on 10.0.0.130 (executor driver) (7/16)
23/08/09 18:57:26 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 17) in 8184 ms on 10.0.0.130 (executor driver) (8/16)
23/08/09 18:57:26 INFO TaskSetManager: Finished task 10.0 in stage 3.0 (TID 26) in 8182 ms on 10.0.0.130 (executor driver) (9/16)
23/08/09 18:57:26 INFO TaskSetManager: Finished task 11.0 in stage 3.0 (TID 27) in 8182 ms on 10.0.0.130 (executor driver) (10/16)
23/08/09 18:57:26 INFO TaskSetManager: Finished task 8.0 in stage 3.0 (TID 24) in 8183 ms on 10.0.0.130 (executor driver) (11/16)
23/08/09 18:57:26 INFO TaskSetManager: Finished task 15.0 in stage 3.0 (TID 31) in 8182 ms on 10.0.0.130 (executor driver) (12/16)
23/08/09 18:57:26 INFO TaskSetManager: Finished task 7.0 in stage 3.0 (TID 23) in 8183 ms on 10.0.0.130 (executor driver) (13/16)
23/08/09 18:57:26 INFO TaskSetManager: Finished task 13.0 in stage 3.0 (TID 29) in 8183 ms on 10.0.0.130 (executor driver) (14/16)
23/08/09 18:57:26 INFO TaskSetManager: Finished task 5.0 in stage 3.0 (TID 21) in 8185 ms on 10.0.0.130 (executor driver) (15/16)
23/08/09 18:57:26 INFO PythonRunner: Times: total = 8196, boot = 8165, init = 31, finish = 0
23/08/09 18:57:26 INFO Executor: Finished task 0.0 in stage 3.0 (TID 16). 2011 bytes result sent to driver
23/08/09 18:57:26 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 16) in 8214 ms on 10.0.0.130 (executor driver) (16/16)
23/08/09 18:57:26 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
23/08/09 18:57:26 INFO DAGScheduler: ResultStage 3 (collect at D:\GitHub\DemoDev\dev-topics-bigdata\dev-topics-sparkinstall\examples\smoketest\smoketest.py:21) finished in 8.222 s
23/08/09 18:57:26 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
23/08/09 18:57:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
23/08/09 18:57:26 INFO DAGScheduler: Job 3 finished: collect at D:\GitHub\DemoDev\dev-topics-bigdata\dev-topics-sparkinstall\examples\smoketest\smoketest.py:21, took 8.223219 s
dataframe : [Row(firstname='James', lastname='Smith', country='USA', state='CA'), Row(firstname='Michael', lastname='Rose', country='USA', state='NY'), Row(firstname='Robert', lastname='Williams', country='USA', state='CA'), Row(firstname='Maria', lastname='Jones', country='USA', state='FL')]
partitions: 16

***************************************
**** Write DataFrame as a CSV file ****
***************************************
  -- File: /tmp/spark_output/smoketest
23/08/09 18:57:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/08/09 18:57:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/08/09 18:57:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/08/09 18:57:26 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0
23/08/09 18:57:26 INFO DAGScheduler: Got job 4 (save at NativeMethodAccessorImpl.java:0) with 1 output partitions
23/08/09 18:57:26 INFO DAGScheduler: Final stage: ResultStage 4 (save at NativeMethodAccessorImpl.java:0)
23/08/09 18:57:26 INFO DAGScheduler: Parents of final stage: List()
23/08/09 18:57:26 INFO DAGScheduler: Missing parents: List()
23/08/09 18:57:26 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[15] at save at NativeMethodAccessorImpl.java:0), which has no missing parents
23/08/09 18:57:26 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 215.8 KiB, free 434.2 MiB)
23/08/09 18:57:26 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 78.4 KiB, free 434.1 MiB)
23/08/09 18:57:26 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.0.130:50445 (size: 78.4 KiB, free: 434.3 MiB)
23/08/09 18:57:26 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1535
23/08/09 18:57:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[15] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
23/08/09 18:57:26 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
23/08/09 18:57:26 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 32) (10.0.0.130, executor driver, partition 0, PROCESS_LOCAL, 8470 bytes) 
23/08/09 18:57:26 INFO Executor: Running task 0.0 in stage 4.0 (TID 32)
23/08/09 18:57:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/08/09 18:57:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/08/09 18:57:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/08/09 18:57:27 INFO PythonRunner: Times: total = 549, boot = 523, init = 26, finish = 0
23/08/09 18:57:28 INFO PythonRunner: Times: total = 544, boot = 517, init = 27, finish = 0
23/08/09 18:57:28 INFO PythonRunner: Times: total = 541, boot = 515, init = 26, finish = 0
23/08/09 18:57:29 INFO PythonRunner: Times: total = 523, boot = 496, init = 27, finish = 0
23/08/09 18:57:29 INFO PythonRunner: Times: total = 551, boot = 524, init = 26, finish = 1
23/08/09 18:57:30 INFO PythonRunner: Times: total = 555, boot = 528, init = 27, finish = 0
23/08/09 18:57:31 INFO PythonRunner: Times: total = 518, boot = 491, init = 27, finish = 0
23/08/09 18:57:31 INFO PythonRunner: Times: total = 523, boot = 497, init = 26, finish = 0
23/08/09 18:57:32 INFO PythonRunner: Times: total = 535, boot = 509, init = 26, finish = 0
23/08/09 18:57:32 INFO PythonRunner: Times: total = 521, boot = 489, init = 32, finish = 0
23/08/09 18:57:33 INFO PythonRunner: Times: total = 558, boot = 532, init = 26, finish = 0
23/08/09 18:57:33 INFO PythonRunner: Times: total = 546, boot = 518, init = 28, finish = 0
23/08/09 18:57:34 INFO PythonRunner: Times: total = 543, boot = 516, init = 27, finish = 0
23/08/09 18:57:34 INFO PythonRunner: Times: total = 536, boot = 510, init = 26, finish = 0
23/08/09 18:57:35 INFO PythonRunner: Times: total = 550, boot = 519, init = 31, finish = 0
23/08/09 18:57:35 INFO PythonRunner: Times: total = 553, boot = 527, init = 25, finish = 1
23/08/09 18:57:35 INFO FileOutputCommitter: Saved output of task 'attempt_202308091857267311761625031131846_0004_m_000000_32' to file:/tmp/spark_output/smoketest/_temporary/0/task_202308091857267311761625031131846_0004_m_000000
23/08/09 18:57:35 INFO SparkHadoopMapRedUtil: attempt_202308091857267311761625031131846_0004_m_000000_32: Committed. Elapsed time: 2 ms.
23/08/09 18:57:35 INFO Executor: Finished task 0.0 in stage 4.0 (TID 32). 3176 bytes result sent to driver
23/08/09 18:57:35 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 32) in 9142 ms on 10.0.0.130 (executor driver) (1/1)
23/08/09 18:57:35 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
23/08/09 18:57:35 INFO DAGScheduler: ResultStage 4 (save at NativeMethodAccessorImpl.java:0) finished in 9.200 s
23/08/09 18:57:35 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
23/08/09 18:57:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
23/08/09 18:57:35 INFO DAGScheduler: Job 4 finished: save at NativeMethodAccessorImpl.java:0, took 9.202988 s
23/08/09 18:57:35 INFO FileFormatWriter: Start to commit write Job 936b7a97-3794-4f5b-bb35-596f06d224b5.
23/08/09 18:57:35 ERROR FileFormatWriter: Aborting job 936b7a97-3794-4f5b-bb35-596f06d224b5.
java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)
	at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)
	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454)
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:833)
Traceback (most recent call last):
  File "D:\GitHub\DemoDev\dev-topics-bigdata\dev-topics-sparkinstall\examples\smoketest\smoketest.py", line 46, in <module>
    smoke2('Create a DataFrame')
  File "D:\GitHub\DemoDev\dev-topics-bigdata\dev-topics-sparkinstall\examples\smoketest\smoketest.py", line 33, in smoke2
    df.coalesce(1).write.format("csv").options(header='True', delimiter=',').mode('overwrite').save(
  File "D:\util\spark-3.4.1-bin-hadoop3-scala2.13\python\lib\pyspark.zip\pyspark\sql\readwriter.py", line 1398, in save
  File "D:\util\spark-3.4.1-bin-hadoop3-scala2.13\python\lib\py4j-0.10.9.7-src.zip\py4j\java_gateway.py", line 1322, in __call__
  File "D:\util\spark-3.4.1-bin-hadoop3-scala2.13\python\lib\pyspark.zip\pyspark\errors\exceptions\captured.py", line 169, in deco
  File "D:\util\spark-3.4.1-bin-hadoop3-scala2.13\python\lib\py4j-0.10.9.7-src.zip\py4j\protocol.py", line 326, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o53.save.
: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)
	at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)
	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454)
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:833)

23/08/09 18:57:36 INFO SparkContext: Invoking stop() from shutdown hook
23/08/09 18:57:36 INFO SparkContext: SparkContext is stopping with exitCode 0.
23/08/09 18:57:36 INFO SparkUI: Stopped Spark web UI at http://10.0.0.130:4040
23/08/09 18:57:36 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/08/09 18:57:36 INFO MemoryStore: MemoryStore cleared
23/08/09 18:57:36 INFO BlockManager: BlockManager stopped
23/08/09 18:57:36 INFO BlockManagerMaster: BlockManagerMaster stopped
23/08/09 18:57:36 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/08/09 18:57:36 INFO SparkContext: Successfully stopped SparkContext
23/08/09 18:57:36 INFO ShutdownHookManager: Shutdown hook called
23/08/09 18:57:36 INFO ShutdownHookManager: Deleting directory D:\Temp\spark-eb120e41-8a16-4827-8de5-b062c29995c2\pyspark-f98e67b2-17df-4ee4-95e7-1ee0ddab2225
23/08/09 18:57:36 INFO ShutdownHookManager: Deleting directory C:\Users\Don\AppData\Local\Temp\spark-480aba60-a353-48a0-b521-30552779e47a
23/08/09 18:57:36 INFO ShutdownHookManager: Deleting directory D:\Temp\spark-eb120e41-8a16-4827-8de5-b062c29995c2

 Submit failed, RC=1

 Smoke Test Done

